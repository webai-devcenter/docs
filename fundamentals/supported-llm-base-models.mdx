---
title: "Supported LLM Base Models"
description: "Navigator supports a variety of open source LLMs, and is adding more all the time."
---

Navigator supports a variety of open source LLMs, and is adding more all the time. Below is the current list of supported models with links to their Hugging Face model pages where you can learn more about each model's strengths and weaknesses.

<Note>
For most use cases that require training or inference on consumer hardware, we recommend using a 7B parameter model. This is the sweet spot between model performance and size for consumer devices. All LLM features in Navigator work well with 7B parameter models.
</Note>

---

## Full List of Supported Models

<AccordionGroup>
  <Accordion title="Codestral Models">
    - [Codestral 22B v0.1 4bit](https://huggingface.co/mlx-community/Codestral-22B-v0.1-4bit)
    - [Codestral 22B v0.1 8bit](https://huggingface.co/mlx-community/Codestral-22B-v0.1-8bit)
  </Accordion>

  <Accordion title="DeepSeek Models">
    - [DeepSeek V3 0324](https://huggingface.co/deepseek-ai/DeepSeek-V3-0324)
    - [DeepSeek V3 0324 8bit](https://huggingface.co/mlx-community/DeepSeek-v3-0324-8bit)
    - [DeepSeek V3 0324 4bit](https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit)
    - [DeepSeek R1 Distill Llama 8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)
    - [DeepSeek R1 4bit](https://huggingface.co/mlx-community/DeepSeek-R1-4bit)
    - [DeepSeek R1 0528 4bit](https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit)
  </Accordion>

  <Accordion title="Gemma Models">
    - [Quantized Gemma 7B IT](https://huggingface.co/mlx-community/quantized-gemma-7b-it)
    - [Gemma 3 1B IT 4bit](https://huggingface.co/mlx-community/gemma-3-1b-it-4bit)
    - [Gemma 3 1B IT 8bit](https://huggingface.co/mlx-community/gemma-3-1b-it-8bit)
    - [Gemma 3 1B IT 6bit](https://huggingface.co/mlx-community/gemma-3-1b-it-6bit)
    - [Gemma 3 1B IT bf16](https://huggingface.co/mlx-community/gemma-3-1b-it-bf16)
    - [Gemma 3 Text 12B IT 4bit](https://huggingface.co/mlx-community/gemma-3-text-12b-it-4bit)
    - [Gemma 3 Text 27B IT 4bit](https://huggingface.co/mlx-community/gemma-3-text-27b-it-4bit)
    - [Gemma 3 4B IT 4bit](https://huggingface.co/mlx-community/gemma-3-4b-it-4bit)
    - [Gemma 3 4B IT bf16](https://huggingface.co/mlx-community/gemma-3-4b-it-bf16)
    - [Gemma 3 12B IT 4bit](https://huggingface.co/mlx-community/gemma-3-12b-it-4bit)
    - [Gemma 3 12B IT 8bit](https://huggingface.co/mlx-community/gemma-3-12b-it-8bit)
    - [Gemma 3 27B IT 4bit](https://huggingface.co/mlx-community/gemma-3-27b-it-4bit)
    - [Gemma 3 27B IT 8bit](https://huggingface.co/mlx-community/gemma-3-27b-it-8bit)
    - [Gemma 3 27B IT QAT bf16](https://huggingface.co/mlx-community/gemma-3-27b-it-qat-bf16)
    - [Gemma 3n E2B IT 3bit](https://huggingface.co/mlx-community/gemma-3n-E2B-it-3bit)
    - [Gemma 3n E2B IT 4bit](https://huggingface.co/mlx-community/gemma-3n-E2B-it-4bit)
    - [Gemma 3n E2B IT 5bit](https://huggingface.co/mlx-community/gemma-3n-E2B-it-5bit)
    - [Gemma 3n E2B IT 6bit](https://huggingface.co/mlx-community/gemma-3n-E2B-it-6bit)
    - [Gemma 3n E2B IT 8bit](https://huggingface.co/mlx-community/gemma-3n-E2B-it-8bit)
    - [Gemma 3n E4B IT 3bit](https://huggingface.co/mlx-community/gemma-3n-E4B-it-3bit)
    - [Gemma 3n E4B IT 5bit](https://huggingface.co/mlx-community/gemma-3n-E4B-it-5bit)
    - [Gemma 3n E4B IT 6bit](https://huggingface.co/mlx-community/gemma-3n-E4B-it-6bit)
    - [Gemma 3n E4B IT 8bit](https://huggingface.co/mlx-community/gemma-3n-E4B-it-8bit)
    - [Gemma SEA-LION v3 9B IT](https://huggingface.co/aisingapore/Gemma-SEA-LION-v3-9B-IT)
    - [CodeGemma 7B IT 8bit](https://huggingface.co/mlx-community/codegemma-7b-it-8bit)
  </Accordion>

  <Accordion title="Llama Models">
    - [Meta Llama 3.1 8B Instruct 4bit](https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-4bit)
    - [Meta Llama 3.1 8B Instruct 8bit](https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-8bit)
    - [Meta Llama 3.1 70B Instruct 4bit](https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit)
    - [Llama 3.1 Nemotron 70B Instruct HF 4bit](https://huggingface.co/mlx-community/nvidia_Llama-3.1-Nemotron-70B-Instruct-HF_4bit)
    - [Llama 3.3 70B Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)
    - [Llama 4 Scout 17B 16E Instruct](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct)
    - [Llama 4 Maverick 17B 128E Instruct](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct)
    - [Llama 4 Maverick 17B 128E Instruct FP8](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8)
    - [Llama 4 Maverick 17B 128E](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E)
    - [Llama 4 Maverick 17B 16E Instruct 6bit](https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-6bit)
    - [Llama 4 Maverick 17B 16E Instruct 4bit](https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit)
    - [Llama 3 Taiwan 70B Instruct](https://huggingface.co/yentinglin/Llama-3-Taiwan-70B-Instruct)
    - [Llama 3.1 Nemotron Nano 8B v1](https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1)
    - [Llama 3.2 1B Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)
    - [Llama 3.2 3B Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)
    - [Llama 3.3 70B Instruct 4bit](https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit)
    - [Llama 3.3 70B Instruct 8bit](https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit)
    - [Llama 4 Scout 17B 16E Instruct 4bit](https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit)
    - [Llama 4 Scout 17B 16E Instruct 8bit](https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit)
    - [Idefics3 8B Llama3 3bit](https://huggingface.co/mlx-community/Idefics3-8B-Llama3-3bit)
    - [Idefics3 8B Llama3 4bit](https://huggingface.co/mlx-community/Idefics3-8B-Llama3-4bit)
    - [Idefics3 8B Llama3 6bit](https://huggingface.co/mlx-community/Idefics3-8B-Llama3-6bit)
    - [Idefics3 8B Llama3 8bit](https://huggingface.co/mlx-community/Idefics3-8B-Llama3-8bit)
    - [Llama SEA-LION v2 8B IT](https://huggingface.co/aisingapore/Llama-SEA-LION-v2-8B-IT)
    - [Llama SEA-LION v3 70B IT](https://huggingface.co/aisingapore/Llama-SEA-LION-v3-70B-IT)
    - [Llama SEA-LION v3 8B IT](https://huggingface.co/aisingapore/Llama-SEA-LION-v3-8B-IT)
    - [Llama SEA-LION v3.5 70B R](https://huggingface.co/aisingapore/Llama-SEA-LION-v3.5-70B-R)
    - [Llama SEA-LION v3.5 8B R](https://huggingface.co/aisingapore/Llama-SEA-LION-v3.5-8B-R)
  </Accordion>

  <Accordion title="Ministral Models">
    - [Ministral 8B Instruct 2410 bf16](https://huggingface.co/mlx-community/Ministral-8B-Instruct-2410-bf16)
  </Accordion>

  <Accordion title="Mistral Models">
    - [Mistral 7B Instruct v0.3](https://huggingface.co/mlx-community/Mistral-7B-Instruct-v0.3)
    - [Mistral NeMo Minitron 8B Instruct](https://huggingface.co/mlx-community/Mistral-NeMo-Minitron-8B-Instruct)
    - [Mistral 7B Instruct v0.3 4bit](https://huggingface.co/mlx-community/Mistral-7B-Instruct-v0.3-4bit)
    - [Mistral 7B Instruct v0.3 8bit](https://huggingface.co/mlx-community/Mistral-7B-Instruct-v0.3-8bit)
    - [Mistral Large Instruct 2407 4bit](https://huggingface.co/mlx-community/Mistral-Large-Instruct-2407-4bit)
    - [Mistral Nemo Instruct 2407 4bit](https://huggingface.co/mlx-community/Mistral-Nemo-Instruct-2407-4bit)
    - [Mistral Small 24B Instruct 2501](https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501)
  </Accordion>

  <Accordion title="Mixtral Models">
    - [Mixtral 8x7B Instruct v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)
    - [Mixtral 8x22B Instruct v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1)
  </Accordion>

  <Accordion title="Phi Models">
    - [Phi 3 Medium 128K Instruct bf16](https://huggingface.co/mlx-community/Phi-3-medium-128k-instruct-bf16)
    - [Phi 3.5 Mini Instruct bf16](https://huggingface.co/mlx-community/Phi-3.5-mini-instruct-bf16)
    - [Phi 4](https://huggingface.co/microsoft/phi-4)
  </Accordion>

  <Accordion title="Qwen Models">
    - [Qwen2 7B Instruct 8bit](https://huggingface.co/mlx-community/Qwen2-7B-Instruct-8bit)
    - [Qwen2 72B Instruct 4bit](https://huggingface.co/mlx-community/Qwen2-72B-Instruct-4bit)
    - [Qwen2.5 72B Instruct 4bit](https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-4bit)
    - [Qwen2.5 14B Instruct](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)
    - [Qwen2.5 Coder 7B Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)
    - [QwQ 32B Preview](https://huggingface.co/Qwen/QwQ-32B-Preview)
    - [QwQ 32B](https://huggingface.co/Qwen/QwQ-32B)
  </Accordion>

  <Accordion title="Qwen3 Models">
    - [Qwen3 0.6B 4bit](https://huggingface.co/mlx-community/Qwen3-0.6B-4bit)
    - [Qwen3 0.6B 6bit](https://huggingface.co/mlx-community/Qwen3-0.6B-6bit)
    - [Qwen3 0.6B 8bit](https://huggingface.co/mlx-community/Qwen3-0.6B-8bit)
    - [Qwen3 0.6B bf16](https://huggingface.co/mlx-community/Qwen3-0.6B-bf16)
    - [Qwen3 1.7B 3bit](https://huggingface.co/mlx-community/Qwen3-1.7B-3bit)
    - [Qwen3 1.7B 4bit](https://huggingface.co/mlx-community/Qwen3-1.7B-4bit)
    - [Qwen3 1.7B 6bit](https://huggingface.co/mlx-community/Qwen3-1.7B-6bit)
    - [Qwen3 1.7B 8bit](https://huggingface.co/mlx-community/Qwen3-1.7B-8bit)
    - [Qwen3 1.7B bf16](https://huggingface.co/mlx-community/Qwen3-1.7B-bf16)
    - [Qwen3 4B 3bit](https://huggingface.co/mlx-community/Qwen3-4B-3bit)
    - [Qwen3 4B 4bit](https://huggingface.co/mlx-community/Qwen3-4B-4bit)
    - [Qwen3 4B 6bit](https://huggingface.co/mlx-community/Qwen3-4B-6bit)
    - [Qwen3 4B 8bit](https://huggingface.co/mlx-community/Qwen3-4B-8bit)
    - [Qwen3 4B bf16](https://huggingface.co/mlx-community/Qwen3-4B-bf16)
    - [Qwen3 8B 3bit](https://huggingface.co/mlx-community/Qwen3-8B-3bit)
    - [Qwen3 8B 4bit](https://huggingface.co/mlx-community/Qwen3-8B-4bit)
    - [Qwen3 8B 6bit](https://huggingface.co/mlx-community/Qwen3-8B-6bit)
    - [Qwen3 1.7B](https://huggingface.co/Qwen/Qwen3-1.7B)
    - [Qwen3 4B](https://huggingface.co/Qwen/Qwen3-4B)
    - [Qwen3 8B](https://huggingface.co/Qwen/Qwen3-8B)
    - [Qwen3 30B A3B 4bit](https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit)
    - [Qwen3 30B A3B 6bit](https://huggingface.co/mlx-community/Qwen3-30B-A3B-6bit)
    - [Qwen3 30B A3B 8bit](https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit)
    - [Qwen3 30B A3B](https://huggingface.co/Qwen/Qwen3-30B-A3B)
    - [Qwen3 235B A22B](https://huggingface.co/Qwen/Qwen3-235B-A22B)
    - [Qwen3 8B 8bit](https://huggingface.co/mlx-community/Qwen3-8B-8bit)
  </Accordion>

  <Accordion title="Other Models">
    - [GPT-OSS 120B MLX 8bit](https://huggingface.co/lmstudio-community/gpt-oss-120b-MLX-8bit)
    - [GPT-OSS 20B MLX 8bit](https://huggingface.co/lmstudio-community/gpt-oss-20b-MLX-8bit)
    - [GPT-OSS 120B 4bit](https://huggingface.co/mlx-community/gpt-oss-120b-4bit)
    - [Sum Small Unquantized](https://huggingface.co/mlx-community/sum-small-unquantized)
    - [SmolVLM2 2.2B Instruct MLX](https://huggingface.co/mlx-community/SmolVLM2-2.2B-Instruct-mlx)
  </Accordion>
</AccordionGroup>

---

## Model Selection Considerations

When selecting a model, consider these key factors:

- **Parameter Count:** Generally, larger models (higher parameter counts) offer better performance but require more computing resources.
- **Memory Requirements:** Each model requires a specific amount of RAM. Ensure your hardware meets these requirements.
- **Specialization:** Some models are optimized for specific tasks (e.g., code generation, instruction following).
- **Quantization:** Many models offer quantized versions (3-bit, 4-bit, 5-bit, 6-bit, 8-bit) that reduce memory requirements at a small cost to performance.

---

## Next Steps

- To get started testing some of these models out, see our [AI Models Guide](/navigator-overview/ai-models-guide)

  For more information on how these LLM's can be utilized through webFrame, see our [webFrame](/fundamentals/webframe) page

